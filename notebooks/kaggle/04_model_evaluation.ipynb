{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“‰ Phase 4 : Ã‰valuation & Robustesse\n",
                "\n",
                "**Objectifs** :\n",
                "1. Ã‰valuer le systÃ¨me hybride sur le jeu de test.\n",
                "2. Analyser les erreurs (Matrices de confusion).\n",
                "3. Tester la **robustesse** face au bruit (simulation de trafic rÃ©el imparfait).\n",
                "4. Comparer les performances par classe d'attaque.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Ajouter le dossier src au path pour importer nos modules\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "from src.hybrid_ids import HybridIDS\n",
                "\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement des DonnÃ©es et du SystÃ¨me"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Charger le systÃ¨me hybride entraÃ®nÃ© (Phase 3)\n",
                "try:\n",
                "    ids = HybridIDS.load('../../models/hybrid_ids_system.pkl')\n",
                "    print(\"âœ… SystÃ¨me Hybride chargÃ© avec succÃ¨s.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"âŒ Erreur : Le modÃ¨le n'est pas trouvÃ©. Avez-vous exÃ©cutÃ© le notebook de Phase 3 ?\")\n",
                "    # En cas d'erreur, on arrÃªte l'exÃ©cution ici si c'Ã©tait un script, mais en notebook on continue pour l'exemple\n",
                "\n",
                "# 2. Charger les donnÃ©es de test (SauvegardÃ©es en Phase 2 ou 3)\n",
                "# Pour cet exemple, on va recharger le CSV nettoyÃ© et refaire le split pour garantir la cohÃ©rence\n",
                "# IdÃ©alement, on aurait sauvegardÃ© X_test.npy et y_test.npy\n",
                "df = pd.read_csv('../../data/processed/cicids2017_cleaned.csv')\n",
                "\n",
                "# RecrÃ©er les labels (Comme en Phase 3)\n",
                "df['Binary_Label'] = (df['Attack Type'] != 'Normal Traffic').astype(int)\n",
                "df['Attack_Merged'] = df['Attack Type'].replace({'DoS': 'DoS_DDoS', 'DDoS': 'DoS_DDoS'})\n",
                "\n",
                "# SÃ©parer Features\n",
                "label_cols = ['Attack Type', 'Binary_Label', 'Attack_Merged', 'Multiclass_Label'] \n",
                "# Note: Multiclass_Label n'existe peut-Ãªtre pas dans le CSV si on ne l'a pas sauvegardÃ©, on l'ignore\n",
                "feature_cols = [col for col in df.columns if col not in label_cols and col not in ['Multiclass_Label']]\n",
                "\n",
                "X = df[feature_cols]\n",
                "y_true_type = df['Attack Type'] # Vrai type pour comparaison finale\n",
                "\n",
                "# Prendre un Ã©chantillon de test (simuler le Test Set)\n",
                "# Dans un vrai projet, on charge X_test sauvegardÃ© pour ne pas tricher\n",
                "from sklearn.model_selection import train_test_split\n",
                "_, X_test, _, y_test = train_test_split(X, y_true_type, test_size=0.2, random_state=42, stratify=y_true_type)\n",
                "\n",
                "print(f\"DonnÃ©es de test prÃªtes : {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ã‰valuation Standard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"ðŸš€ Lancement des prÃ©dictions sur le jeu de test...\")\n",
                "results = ids.predict_df(X_test)\n",
                "\n",
                "# Ajouter les vrais labels pour comparer\n",
                "results['True_Label'] = y_test.values\n",
                "\n",
                "results.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mapping pour aligner les noms (Le modÃ¨le a peut-Ãªtre fusionnÃ© DoS/DDoS)\n",
                "# On doit s'assurer que les labels prÃ©dits correspondent aux labels vrais pour le rapport\n",
                "# Le modÃ¨le sort 'DoS_DDoS', mais le vrai label est 'DoS' ou 'DDoS'.\n",
                "results['True_Label_Merged'] = results['True_Label'].replace({'DoS': 'DoS_DDoS', 'DDoS': 'DoS_DDoS'})\n",
                "\n",
                "# Calcul des mÃ©triques globales\n",
                "accuracy = accuracy_score(results['True_Label_Merged'], results['type'])\n",
                "print(f\"ðŸŽ¯ Accuracy Globale : {accuracy:.2%}\")\n",
                "\n",
                "print(\"\\nðŸ“Š Rapport de Classification DÃ©taillÃ© :\")\n",
                "print(classification_report(results['True_Label_Merged'], results['type']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Matrice de Confusion Visuelle\n",
                "plt.figure(figsize=(12, 10))\n",
                "cm = confusion_matrix(results['True_Label_Merged'], results['type'])\n",
                "labels = sorted(results['True_Label_Merged'].unique())\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
                "plt.title('Matrice de Confusion (SystÃ¨me Hybride)')\n",
                "plt.ylabel('Vraie Classe')\n",
                "plt.xlabel('Classe PrÃ©dite')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test de Robustesse (Simulation de Bruit)\n",
                "\n",
                "Dans la rÃ©alitÃ©, le trafic n'est jamais aussi propre que dans le dataset CICIDS2017.\n",
                "Nous allons ajouter du **bruit gaussien** aux donnÃ©es pour voir si le modÃ¨le tient le coup."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_noise(X, noise_level=0.1):\n",
                "    \"\"\"\n",
                "    Ajoute du bruit alÃ©atoire aux features.\n",
                "    noise_level: Ecart-type du bruit (proportionnel Ã  l'Ã©cart-type de la feature)\n",
                "    \"\"\"\n",
                "    noise = np.random.normal(0, noise_level, X.shape)\n",
                "    # On multiplie par l'Ã©cart-type de chaque colonne pour que le bruit soit Ã  l'Ã©chelle\n",
                "    X_noisy = X + noise * X.std().values\n",
                "    return X_noisy\n",
                "\n",
                "# Test avec diffÃ©rents niveaux de bruit\n",
                "noise_levels = [0.0, 0.1, 0.2, 0.5] # 0%, 10%, 20%, 50% de bruit\n",
                "accuracies = []\n",
                "\n",
                "print(\"ðŸ›¡ï¸ Test de Robustesse en cours...\")\n",
                "for nl in noise_levels:\n",
                "    X_noisy = add_noise(X_test, noise_level=nl)\n",
                "    res_noisy = ids.predict_df(X_noisy)\n",
                "    acc = accuracy_score(results['True_Label_Merged'], res_noisy['type'])\n",
                "    accuracies.append(acc)\n",
                "    print(f\"   Bruit {nl*100}% -> Accuracy: {acc:.2%}\")\n",
                "\n",
                "# Visualisation de la chute de performance\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.plot(noise_levels, accuracies, marker='o', linestyle='-', color='red')\n",
                "plt.title('Robustesse du ModÃ¨le face au Bruit')\n",
                "plt.xlabel('Niveau de Bruit (Noise Level)')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyse des Erreurs\n",
                "\n",
                "OÃ¹ le modÃ¨le se trompe-t-il le plus ? C'est crucial pour l'amÃ©lioration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filtrer les erreurs\n",
                "errors = results[results['True_Label_Merged'] != results['type']]\n",
                "\n",
                "print(f\"Nombre total d'erreurs : {len(errors)} sur {len(results)} prÃ©dictions.\")\n",
                "print(\"Top 5 des confusions (Vrai -> PrÃ©dit) :\")\n",
                "confusion_counts = errors.groupby(['True_Label_Merged', 'type']).size().sort_values(ascending=False).head(5)\n",
                "print(confusion_counts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Conclusion de l'Ã‰valuation\n",
                "\n",
                "**RÃ©sumÃ© :**\n",
                "- Si l'accuracy reste > 90% mÃªme avec un peu de bruit, le modÃ¨le est robuste.\n",
                "- Regardez la matrice de confusion : est-ce qu'on confond souvent `DoS` et `PortScan` ? C'est moins grave que de confondre `Attaque` et `Normal`.\n",
                "\n",
                "**Prochaine Ã©tape :** Simulation Temps RÃ©el !"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
