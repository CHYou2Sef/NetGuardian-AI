{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç CICIDS2017 - Data Exploration & Cleaning\n",
    "\n",
    "**Objectif** : Explorer et nettoyer le dataset CICIDS2017 sur Kaggle\n",
    "\n",
    "**Dataset** : https://www.kaggle.com/datasets/cicdataset/cicids2017\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Charger le Dataset\n",
    "\n",
    "Commen√ßons par un seul fichier pour l'exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un fichier (Monday - trafic normal uniquement)\n",
    "df = pd.read_csv('/kaggle/input/cicids2017/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Taille en m√©moire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premi√®res lignes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations g√©n√©rales\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des labels\n",
    "print(\"Distribution des labels:\")\n",
    "print(df[' Label'].value_counts())\n",
    "print(f\"\\nPourcentages:\")\n",
    "print(df[' Label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "df[' Label'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution des Labels - CICIDS2017', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Type d\\'Attaque', fontsize=12)\n",
    "plt.ylabel('Nombre d\\'Instances', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ D√©tection des Probl√®mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. V√©rifier les valeurs manquantes\n",
    "print(\"=\" * 50)\n",
    "print(\"1. VALEURS MANQUANTES (NaN)\")\n",
    "print(\"=\" * 50)\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "else:\n",
    "    print(\"‚úÖ Aucune valeur NaN d√©tect√©e\")\n",
    "\n",
    "print(f\"\\nTotal NaN: {nan_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. V√©rifier les valeurs infinies\n",
    "print(\"=\" * 50)\n",
    "print(\"2. VALEURS INFINIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "inf_counts = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    for col, count in sorted(inf_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{col}: {count}\")\n",
    "else:\n",
    "    print(\"‚úÖ Aucune valeur infinie d√©tect√©e\")\n",
    "\n",
    "print(f\"\\nTotal colonnes avec infinis: {len(inf_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. V√©rifier les duplications\n",
    "print(\"=\" * 50)\n",
    "print(\"3. DUPLICATIONS\")\n",
    "print(\"=\" * 50)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Nombre de lignes dupliqu√©es: {duplicates}\")\n",
    "print(f\"Pourcentage: {duplicates / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. V√©rifier les noms de colonnes\n",
    "print(\"=\" * 50)\n",
    "print(\"4. NOMS DE COLONNES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nombre de colonnes: {len(df.columns)}\")\n",
    "print(\"\\nColonnes avec espaces:\")\n",
    "for col in df.columns:\n",
    "    if col.startswith(' ') or col.endswith(' '):\n",
    "        print(f\"  - '{col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Fonction de Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cicids2017(df):\n",
    "    \"\"\"\n",
    "    Nettoie le dataset CICIDS2017\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame pandas\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame nettoy√©\n",
    "    \"\"\"\n",
    "    print(\"üßπ Nettoyage en cours...\")\n",
    "    print(f\"Shape initiale: {df.shape}\")\n",
    "    \n",
    "    # 1. Nettoyer les noms de colonnes\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "    print(\"‚úÖ Noms de colonnes nettoy√©s\")\n",
    "    \n",
    "    # 2. Supprimer les duplications\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    print(f\"‚úÖ Duplications supprim√©es: {duplicates_removed}\")\n",
    "    \n",
    "    # 3. G√©rer les valeurs infinies\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(\"‚úÖ Valeurs infinies remplac√©es par NaN\")\n",
    "    \n",
    "    # 4. G√©rer les NaN\n",
    "    nan_before = df.isnull().sum().sum()\n",
    "    \n",
    "    # Remplir avec la m√©diane pour les colonnes num√©riques\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    nan_after = df.isnull().sum().sum()\n",
    "    print(f\"‚úÖ NaN trait√©s: {nan_before} ‚Üí {nan_after}\")\n",
    "    \n",
    "    # 5. Corriger les valeurs n√©gatives incorrectes\n",
    "    positive_cols = ['Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets']\n",
    "    for col in positive_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                df.loc[df[col] < 0, col] = 0\n",
    "                print(f\"‚úÖ {col}: {negative_count} valeurs n√©gatives corrig√©es\")\n",
    "    \n",
    "    # 6. Supprimer les colonnes non pertinentes pour ML\n",
    "    cols_to_drop = ['Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp']\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        df = df.drop(columns=existing_cols_to_drop)\n",
    "        print(f\"‚úÖ Colonnes supprim√©es: {existing_cols_to_drop}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Nettoyage termin√©!\")\n",
    "    print(f\"Shape finale: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Appliquer le Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer le dataset\n",
    "df_clean = clean_cicids2017(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le r√©sultat\n",
    "print(\"V√©rification post-nettoyage:\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"NaN: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Infinis: {np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Duplications: {df_clean.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Mapping des Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de mapping\n",
    "ATTACK_CATEGORIES = {\n",
    "    'BENIGN': 'Normal',\n",
    "    'FTP-Patator': 'Brute_Force',\n",
    "    'SSH-Patator': 'Brute_Force',\n",
    "    'Web Attack ‚Äì Brute Force': 'Brute_Force',\n",
    "    'DoS slowloris': 'DoS_DDoS',\n",
    "    'DoS Slowhttptest': 'DoS_DDoS',\n",
    "    'DoS Hulk': 'DoS_DDoS',\n",
    "    'DoS GoldenEye': 'DoS_DDoS',\n",
    "    'DDoS': 'DoS_DDoS',\n",
    "    'Web Attack ‚Äì XSS': 'Web_Attack',\n",
    "    'Web Attack ‚Äì SQL Injection': 'Web_Attack',\n",
    "    'PortScan': 'Reconnaissance',\n",
    "    'Bot': 'Botnet',\n",
    "    'Infiltration': 'Advanced_Threat',\n",
    "    'Heartbleed': 'Advanced_Threat'\n",
    "}\n",
    "\n",
    "# Appliquer le mapping\n",
    "df_clean['Attack_Category'] = df_clean['Label'].map(ATTACK_CATEGORIES)\n",
    "\n",
    "# V√©rifier\n",
    "print(\"Distribution des cat√©gories d'attaques:\")\n",
    "print(df_clean['Attack_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder num√©riquement\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_clean['Label_Encoded'] = le.fit_transform(df_clean['Attack_Category'])\n",
    "\n",
    "# Afficher le mapping\n",
    "print(\"Mapping num√©rique:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarder le Dataset Nettoy√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder\n",
    "output_file = 'cicids2017_monday_cleaned.csv'\n",
    "df_clean.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Dataset sauvegard√©: {output_file}\")\n",
    "print(f\"Taille: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Traiter Tous les Fichiers (Optionnel)\n",
    "\n",
    "‚ö†Ô∏è **Attention** : Cela peut prendre beaucoup de temps et de m√©moire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de tous les fichiers\n",
    "files = [\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "# Traiter tous les fichiers\n",
    "all_dfs = []\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Traitement de: {file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Charger\n",
    "        df_temp = pd.read_csv(f'/kaggle/input/cicids2017/{file}')\n",
    "        \n",
    "        # Nettoyer\n",
    "        df_temp_clean = clean_cicids2017(df_temp)\n",
    "        \n",
    "        # Mapper les labels\n",
    "        df_temp_clean['Attack_Category'] = df_temp_clean['Label'].map(ATTACK_CATEGORIES)\n",
    "        \n",
    "        # Ajouter √† la liste\n",
    "        all_dfs.append(df_temp_clean)\n",
    "        \n",
    "        print(f\"‚úÖ {file} trait√© avec succ√®s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur avec {file}: {e}\")\n",
    "\n",
    "# Combiner tous les DataFrames\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Combinaison de tous les fichiers...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_final = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nüéâ Dataset final combin√©!\")\n",
    "print(f\"Shape: {df_final.shape}\")\n",
    "print(f\"\\nDistribution finale des labels:\")\n",
    "print(df_final['Attack_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les labels finaux\n",
    "le_final = LabelEncoder()\n",
    "df_final['Label_Encoded'] = le_final.fit_transform(df_final['Attack_Category'])\n",
    "\n",
    "# Sauvegarder le dataset complet\n",
    "df_final.to_csv('cicids2017_full_cleaned.csv', index=False)\n",
    "print(\"‚úÖ Dataset complet sauvegard√©: cicids2017_full_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Prochaines √âtapes\n",
    "\n",
    "1. ‚úÖ Dataset explor√© et nettoy√©\n",
    "2. ‚úÖ Labels mapp√©s en cat√©gories\n",
    "3. ‚úÖ Fichier CSV sauvegard√©\n",
    "\n",
    "**√Ä faire ensuite :**\n",
    "- T√©l√©charger le fichier depuis Kaggle Output\n",
    "- Placer dans `data/processed/` de votre projet local\n",
    "- Commit sur Git\n",
    "- Passer au Feature Engineering et Training !\n",
    "\n",
    "---\n",
    "\n",
    "**Bon courage ! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
