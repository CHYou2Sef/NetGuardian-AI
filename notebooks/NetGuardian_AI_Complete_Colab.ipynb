{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è NetGuardian-AI: Complete IDS Pipeline\n",
    "\n",
    "**Comprehensive All-in-One Google Colab Notebook**\n",
    "\n",
    "This notebook consolidates the entire NetGuardian-AI Intrusion Detection System workflow:\n",
    "1. Dataset Construction & Cleaning\n",
    "2. MITRE ATT&CK Analysis\n",
    "3. Data Preparation & Feature Engineering\n",
    "4. Hybrid Model Training (Binary + Multi-Class)\n",
    "5. Model Evaluation & Robustness Testing\n",
    "6. Real-Time Simulation\n",
    "7. Model Comparison Benchmark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "- [Phase 0: Environment Setup](#phase0)\n",
    "- [Phase 1: Dataset Construction](#phase1)\n",
    "- [Phase 2: MITRE Analysis](#phase2)\n",
    "- [Phase 3: Data Preparation](#phase3)\n",
    "- [Phase 4: Hybrid Model Training](#phase4)\n",
    "- [Phase 5: Model Evaluation](#phase5)\n",
    "- [Phase 6: Real-Time Simulation](#phase6)\n",
    "- [Phase 7: Model Comparison](#phase7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase0'></a>\n",
    "## üì¶ Phase 0: Environment Setup\n",
    "\n",
    "**Purpose**: Install required libraries and configure the environment.\n",
    "\n",
    "**What this does**:\n",
    "- Installs machine learning libraries (XGBoost, imbalanced-learn)\n",
    "- Imports all necessary Python packages\n",
    "- Configures visualization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q xgboost imbalanced-learn\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score, precision_score, \n",
    "    recall_score, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# XGBoost and imbalanced-learn\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TensorFlow for Autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"‚úÖ All libraries imported!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase1'></a>\n",
    "## üîç Phase 1: Dataset Construction & Cleaning\n",
    "\n",
    "**Purpose**: Load and clean the CICIDS2017 dataset.\n",
    "\n",
    "**Dataset**: CICIDS2017 from Kaggle\n",
    "- URL: https://www.kaggle.com/datasets/cicdataset/cicids2017\n",
    "\n",
    "**What this phase does**:\n",
    "1. Loads the raw CICIDS2017 dataset\n",
    "2. Explores data structure and statistics\n",
    "3. Detects and fixes data quality issues (NaN, infinites, duplicates)\n",
    "4. Cleans column names\n",
    "5. Maps attack types to categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Load Dataset\n",
    "\n",
    "**Explanation**: We start by loading one file from CICIDS2017 to explore its structure. The dataset contains network flow features captured from real network traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-cleaned dataset from Kaggle\n",
    "# If using raw CICIDS2017, use: /kaggle/input/cicids2017/Monday-WorkingHours.pcap_ISCX.csv\n",
    "df = pd.read_csv('/kaggle/input/cicids2017-cleaned-and-preprocessed/cicids2017_cleaned.csv')\n",
    "\n",
    "print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"üìù Number of Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Initial Exploration\n",
    "\n",
    "**Explanation**: Understanding the data structure helps us identify potential issues and plan our preprocessing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Show data types and non-null counts\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Analyze Attack Distribution\n",
    "\n",
    "**Explanation**: Understanding the distribution of attack types helps us identify class imbalance issues that we'll need to address during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of attack types\n",
    "print(\"Attack Type Distribution:\")\n",
    "print(df['Attack Type'].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print(df['Attack Type'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "df['Attack Type'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Attack Types - CICIDS2017', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Attack Type', fontsize=12)\n",
    "plt.ylabel('Number of Instances', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Data Quality Checks\n",
    "\n",
    "**Explanation**: We check for common data quality issues:\n",
    "- **NaN values**: Missing data that needs imputation\n",
    "- **Infinite values**: Result of division by zero or overflow\n",
    "- **Duplicates**: Redundant rows that can bias the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*50)\n",
    "print(\"1. MISSING VALUES (NaN)\")\n",
    "print(\"=\"*50)\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No NaN values detected\")\n",
    "print(f\"\\nTotal NaN: {nan_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"=\"*50)\n",
    "print(\"2. INFINITE VALUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "inf_counts = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    for col, count in sorted(inf_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{col}: {count:,}\")\n",
    "else:\n",
    "    print(\"‚úÖ No infinite values detected\")\n",
    "\n",
    "print(f\"\\nTotal columns with infinites: {len(inf_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"=\"*50)\n",
    "print(\"3. DUPLICATE ROWS\")\n",
    "print(\"=\"*50)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates:,}\")\n",
    "print(f\"Percentage: {duplicates / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Data Cleaning Function\n",
    "\n",
    "**Explanation**: This function performs comprehensive cleaning:\n",
    "1. **Strip column names**: Removes leading/trailing spaces\n",
    "2. **Remove duplicates**: Eliminates redundant rows\n",
    "3. **Handle infinites**: Replaces inf/-inf with NaN\n",
    "4. **Impute NaN**: Fills missing values with median (robust to outliers)\n",
    "5. **Fix negative values**: Corrects impossible negative values in certain features\n",
    "6. **Drop irrelevant columns**: Removes non-predictive features (IPs, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cicids2017(df):\n",
    "    \"\"\"\n",
    "    Clean CICIDS2017 dataset\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üßπ Starting data cleaning...\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # 1. Clean column names (remove spaces)\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "    print(\"‚úÖ Column names cleaned\")\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    print(f\"‚úÖ Duplicates removed: {duplicates_removed:,}\")\n",
    "    \n",
    "    # 3. Replace infinite values with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(\"‚úÖ Infinite values replaced with NaN\")\n",
    "    \n",
    "    # 4. Fill NaN with median for numeric columns\n",
    "    nan_before = df.isnull().sum().sum()\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    nan_after = df.isnull().sum().sum()\n",
    "    print(f\"‚úÖ NaN handled: {nan_before:,} ‚Üí {nan_after:,}\")\n",
    "    \n",
    "    # 5. Fix negative values in features that should be positive\n",
    "    positive_cols = ['Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets']\n",
    "    for col in positive_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                df.loc[df[col] < 0, col] = 0\n",
    "                print(f\"‚úÖ {col}: {negative_count:,} negative values fixed\")\n",
    "    \n",
    "    print(f\"\\nüéâ Cleaning complete!\")\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning\n",
    "df_clean = clean_cicids2017(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.6: Verify Cleaning Results\n",
    "\n",
    "**Explanation**: We verify that all data quality issues have been resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"Post-cleaning verification:\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"NaN count: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Infinite count: {np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Duplicate count: {df_clean.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase2'></a>\n",
    "## üïµÔ∏è Phase 2: MITRE ATT&CK Analysis\n",
    "\n",
    "**Purpose**: Map attacks to the MITRE ATT&CK framework for better understanding.\n",
    "\n",
    "**What this phase does**:\n",
    "- Maps CICIDS2017 attacks to MITRE ATT&CK tactics and techniques\n",
    "- Analyzes network characteristics (ports, protocols) of each attack type\n",
    "- Provides cybersecurity context for the detected attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MITRE ATT&CK Mapping\n",
    "\n",
    "**Explanation**: The MITRE ATT&CK framework is a globally-accessible knowledge base of adversary tactics and techniques. Mapping our attacks helps us understand:\n",
    "- **What the attacker is trying to achieve** (Tactic)\n",
    "- **How they're doing it** (Technique)\n",
    "\n",
    "| Attack Type | MITRE Tactic | Technique ID | Description |\n",
    "|-------------|--------------|--------------|-------------|\n",
    "| **Brute Force (FTP/SSH)** | Credential Access | T1110 | Trying multiple passwords to gain access |\n",
    "| **DoS / DDoS** | Impact | T1498 | Overwhelming network resources to cause unavailability |\n",
    "| **Port Scanning** | Discovery | T1046 | Searching for open ports and services |\n",
    "| **Web Attacks (SQL/XSS)** | Initial Access | T1190 | Exploiting web application vulnerabilities |\n",
    "| **Botnet** | Command and Control | T1043 | Zombie machines controlled remotely |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Port Analysis\n",
    "\n",
    "**Explanation**: Different attacks target specific ports:\n",
    "- **SSH Brute Force** ‚Üí Port 22\n",
    "- **FTP Brute Force** ‚Üí Port 21\n",
    "- **Web Attacks** ‚Üí Ports 80 (HTTP) and 443 (HTTPS)\n",
    "\n",
    "This analysis helps validate that our dataset contains realistic attack patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze port targeting by attack type\n",
    "# Note: This requires 'Destination_Port' column in the dataset\n",
    "if 'Destination_Port' in df_clean.columns:\n",
    "    attacks = df_clean[df_clean['Attack_Type'] != 'Normal_Traffic']\n",
    "    top_ports = attacks.groupby(['Attack_Type', 'Destination_Port']).size().reset_index(name='count')\n",
    "    top_ports = top_ports.sort_values(['Attack_Type', 'count'], ascending=[True, False]).groupby('Attack_Type').head(3)\n",
    "    \n",
    "    print(\"Top 3 ports targeted by each attack type:\")\n",
    "    display(top_ports)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Port information not available in this dataset version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase3'></a>\n",
    "## üéØ Phase 3: Data Preparation\n",
    "\n",
    "**Purpose**: Prepare data for machine learning.\n",
    "\n",
    "**What this phase does**:\n",
    "1. Creates binary labels (Normal vs Attack)\n",
    "2. Creates multi-class labels (specific attack types)\n",
    "3. Encodes categorical labels to numeric\n",
    "4. Separates features from labels\n",
    "5. Normalizes features using StandardScaler\n",
    "6. Saves preprocessed data and encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Create Binary Labels\n",
    "\n",
    "**Explanation**: Binary classification is simpler and faster. It answers: \"Is this traffic malicious?\"\n",
    "- **0** = Normal Traffic\n",
    "- **1** = Attack (any type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels\n",
    "df_clean['Binary_Label'] = (df_clean['Attack_Type'] != 'Normal_Traffic').astype(int)\n",
    "\n",
    "print(\"Binary Label Distribution:\")\n",
    "print(df_clean['Binary_Label'].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print(df_clean['Binary_Label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Create Multi-Class Labels\n",
    "\n",
    "**Explanation**: Multi-class classification identifies the specific attack type. We merge DoS and DDoS since they're similar attacks (both aim to overwhelm resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DoS and DDoS into one category\n",
    "df_clean['Attack_Merged'] = df_clean['Attack_Type'].replace({\n",
    "    'DoS': 'DoS_DDoS',\n",
    "    'DDoS': 'DoS_DDoS'\n",
    "})\n",
    "\n",
    "print(\"Attack Type Distribution (after merging):\")\n",
    "print(df_clean['Attack_Merged'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Encode Labels\n",
    "\n",
    "**Explanation**: Machine learning models require numeric labels. LabelEncoder converts text labels to integers (0, 1, 2, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode multi-class labels\n",
    "le = LabelEncoder()\n",
    "df_clean['Multiclass_Label'] = le.fit_transform(df_clean['Attack_Merged'])\n",
    "\n",
    "# Display mapping\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    count = (df_clean['Multiclass_Label'] == i).sum()\n",
    "    print(f\"{i}: {label:20s} ({count:,} instances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Visualize Label Distributions\n",
    "\n",
    "**Explanation**: Visualizing helps us understand class imbalance, which we'll address with SMOTE during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Binary distribution\n",
    "df_clean['Binary_Label'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Binary Classification', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Label (0=Normal, 1=Attack)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Normal', 'Attack'], rotation=0)\n",
    "\n",
    "# Multi-class distribution\n",
    "df_clean['Attack_Merged'].value_counts().plot(kind='bar', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Multi-Class Classification', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Attack Type')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5: Prepare Features\n",
    "\n",
    "**Explanation**: We separate features (X) from labels (y). Features are the network flow characteristics used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "label_cols = ['Attack_Type', 'Binary_Label', 'Attack_Merged', 'Multiclass_Label']\n",
    "feature_cols = [col for col in df_clean.columns if col not in label_cols]\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y_binary = df_clean['Binary_Label']\n",
    "y_multiclass = df_clean['Multiclass_Label']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Binary labels shape: {y_binary.shape}\")\n",
    "print(f\"Multi-class labels shape: {y_multiclass.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.6: Train/Test Split\n",
    "\n",
    "**Explanation**: We split data into training (80%) and testing (20%) sets. Stratification ensures both sets have the same class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_binary_train, y_binary_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Get corresponding multi-class labels\n",
    "y_multi_train = y_multiclass.loc[X_train.index]\n",
    "y_multi_test = y_multiclass.loc[X_test.index]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(y_binary_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.7: Feature Normalization\n",
    "\n",
    "**Explanation**: StandardScaler normalizes features to have mean=0 and std=1. This is crucial for:\n",
    "- **Faster convergence** in gradient-based algorithms\n",
    "- **Equal feature importance** (prevents features with large values from dominating)\n",
    "- **Better performance** in distance-based algorithms (KNN, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features normalized\")\n",
    "print(f\"Mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.8: Save Preprocessed Data\n",
    "\n",
    "**Explanation**: We save the scaler and label encoder to ensure consistent preprocessing during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data and encoders\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "print(\"‚úÖ Scaler and encoder saved\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(df_clean):,}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {len(le.classes_)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    count = (y_multiclass == i).sum()\n",
    "    pct = (count / len(y_multiclass)) * 100\n",
    "    print(f\"  {i}: {label:20s} {count:8,} ({pct:5.2f}%)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
