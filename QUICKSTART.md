# üöÄ Quick Start Guide - NetGuardian-AI

## üìã Pr√©requis

Avant de commencer, assurez-vous d'avoir :
- ‚úÖ Python 3.10 ou sup√©rieur install√©
- ‚úÖ pip (gestionnaire de paquets Python)
- ‚úÖ Git (optionnel, pour le versioning)
- ‚úÖ Compte Kaggle (pour acc√©der au dataset CICIDS2017)
- ‚úÖ Compte Google (pour utiliser Colab avec GPU)

---

## ‚ö° Installation Rapide

### 1. Naviguer vers le projet
```bash
cd "y:\ENICar\cours\5th Sem\CybSec\project\NetGuardian-AI"
```

### 2. Cr√©er un environnement virtuel
```bash
python -m venv venv
```

### 3. Activer l'environnement
```bash
# Windows
.\venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 4. Installer les d√©pendances
```bash
pip install -r requirements.txt
```

---

## üìä Obtenir le Dataset

### Option 1 : Kaggle (Recommand√©)

1. **Cr√©er un compte Kaggle** : https://www.kaggle.com/
2. **Acc√©der au dataset** : https://www.kaggle.com/datasets/cicdataset/cicids2017
3. **T√©l√©charger** ou **utiliser directement dans un notebook Kaggle**

### Option 2 : Source Officielle

1. Visiter : https://www.unb.ca/cic/datasets/ids-2017.html
2. T√©l√©charger les fichiers CSV
3. Placer dans `data/raw/`

---

## üéØ Workflow Recommand√©

### Phase 1 : Exploration (Kaggle)

1. **Cr√©er un notebook Kaggle**
   - Aller sur https://www.kaggle.com/code
   - Nouveau notebook ‚Üí Ajouter dataset CICIDS2017
   - Activer GPU : Settings ‚Üí Accelerator ‚Üí GPU

2. **Explorer les donn√©es**
   ```python
   import pandas as pd
   
   # Charger le dataset
   df = pd.read_csv('/kaggle/input/cicids2017/...')
   
   # Explorer
   print(df.head())
   print(df.info())
   print(df['Label'].value_counts())
   ```

3. **Sauvegarder les insights**
   - Cr√©er des visualisations
   - Noter les observations

### Phase 2 : Pr√©paration (Kaggle ‚Üí Local)

1. **Nettoyer les donn√©es sur Kaggle**
   ```python
   # Supprimer les duplications
   df = df.drop_duplicates()
   
   # G√©rer les valeurs manquantes
   df = df.dropna()
   
   # Sauvegarder
   df.to_csv('cicids2017_cleaned.csv', index=False)
   ```

2. **T√©l√©charger le dataset nettoy√©**
   - Cliquer sur "Output" dans Kaggle
   - T√©l√©charger `cicids2017_cleaned.csv`
   - Placer dans `data/processed/`

3. **Uploader sur Google Drive** (pour Colab)
   - Cr√©er dossier `NetGuardian-AI/data/processed/`
   - Uploader le CSV nettoy√©

### Phase 3 : Training ML (Local)

1. **Lancer Jupyter Notebook**
   ```bash
   jupyter notebook
   ```

2. **Cr√©er un notebook** : `notebooks/local/03_ml_training.ipynb`

3. **Entra√Æner les mod√®les**
   ```python
   from sklearn.ensemble import RandomForestClassifier
   import joblib
   
   # Charger les donn√©es
   df = pd.read_csv('data/processed/cicids2017_cleaned.csv')
   
   # Pr√©parer X et y
   X = df.drop('Label', axis=1)
   y = df['Label']
   
   # Entra√Æner Random Forest
   rf = RandomForestClassifier(n_estimators=100)
   rf.fit(X_train, y_train)
   
   # Sauvegarder
   joblib.dump(rf, 'models/ml/random_forest.pkl')
   ```

### Phase 4 : Training DL (Colab)

1. **Ouvrir Google Colab** : https://colab.research.google.com/

2. **Cr√©er un nouveau notebook** : `NetGuardian_DL_Training.ipynb`

3. **Monter Google Drive**
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

4. **V√©rifier GPU**
   ```python
   import tensorflow as tf
   print("GPU Available:", tf.config.list_physical_devices('GPU'))
   ```

5. **Entra√Æner LSTM/Autoencoder**
   ```python
   # Charger donn√©es depuis Drive
   df = pd.read_csv('/content/drive/MyDrive/NetGuardian-AI/data/processed/cicids2017_cleaned.csv')
   
   # Construire mod√®le LSTM
   from tensorflow.keras.models import Sequential
   from tensorflow.keras.layers import LSTM, Dense
   
   model = Sequential([
       LSTM(64, input_shape=(sequence_length, n_features)),
       Dense(32, activation='relu'),
       Dense(n_classes, activation='softmax')
   ])
   
   model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
   model.fit(X_train, y_train, epochs=50, batch_size=256)
   
   # Sauvegarder sur Drive
   model.save('/content/drive/MyDrive/NetGuardian-AI/models/dl/lstm_model.h5')
   ```

6. **T√©l√©charger le mod√®le en local**
   - Depuis Google Drive
   - Placer dans `models/dl/`

### Phase 5 : √âvaluation (Local)

1. **Cr√©er notebook** : `notebooks/local/04_evaluation.ipynb`

2. **Charger tous les mod√®les**
   ```python
   import joblib
   from tensorflow.keras.models import load_model
   
   # ML models
   rf = joblib.load('models/ml/random_forest.pkl')
   xgb = joblib.load('models/ml/xgboost.pkl')
   
   # DL models
   lstm = load_model('models/dl/lstm_model.h5')
   ```

3. **Comparer les performances**
   ```python
   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
   
   models = {'RF': rf, 'XGB': xgb, 'LSTM': lstm}
   results = {}
   
   for name, model in models.items():
       y_pred = model.predict(X_test)
       results[name] = {
           'accuracy': accuracy_score(y_test, y_pred),
           'precision': precision_score(y_test, y_pred, average='weighted'),
           'recall': recall_score(y_test, y_pred, average='weighted'),
           'f1': f1_score(y_test, y_pred, average='weighted')
       }
   
   print(pd.DataFrame(results).T)
   ```

### Phase 6 : Dashboard (Local)

1. **Lancer le dashboard**
   ```bash
   streamlit run app/dashboard.py
   ```

2. **Ouvrir le navigateur**
   - URL : http://localhost:8501

3. **Tester la d√©tection**
   - Uploader un √©chantillon de donn√©es
   - Voir les pr√©dictions en temps r√©el

---

## üê≥ D√©ploiement Docker (Optionnel)

### Build et Run
```bash
# Build l'image
docker-compose build

# Lancer le conteneur
docker-compose up

# Acc√©der au dashboard
# http://localhost:8501
```

### Arr√™ter
```bash
docker-compose down
```

---

## üìù Checklist de D√©marrage

### Setup Initial
- [ ] Python 3.10+ install√©
- [ ] Environnement virtuel cr√©√© et activ√©
- [ ] D√©pendances install√©es (`pip install -r requirements.txt`)
- [ ] Compte Kaggle cr√©√©
- [ ] Compte Google cr√©√©

### Dataset
- [ ] Dataset CICIDS2017 t√©l√©charg√©
- [ ] Dataset explor√© sur Kaggle
- [ ] Dataset nettoy√©
- [ ] Dataset upload√© sur Google Drive

### Training
- [ ] Mod√®les ML entra√Æn√©s en local
- [ ] Mod√®les DL entra√Æn√©s sur Colab
- [ ] Mod√®les sauvegard√©s

### √âvaluation
- [ ] M√©triques calcul√©es
- [ ] Mod√®les compar√©s
- [ ] Meilleur mod√®le s√©lectionn√©

### Dashboard
- [ ] Dashboard lanc√©
- [ ] D√©tection test√©e
- [ ] Alertes v√©rifi√©es

---

## üÜò Troubleshooting

### Probl√®me : Erreur d'installation de d√©pendances
**Solution** :
```bash
pip install --upgrade pip
pip install -r requirements.txt --no-cache-dir
```

### Probl√®me : Dataset trop volumineux
**Solution** : Utiliser un √©chantillon
```python
df_sample = df.sample(frac=0.1, random_state=42)  # 10% du dataset
```

### Probl√®me : Manque de m√©moire
**Solution** : Charger par chunks
```python
chunks = pd.read_csv('data.csv', chunksize=10000)
for chunk in chunks:
    process(chunk)
```

### Probl√®me : Colab se d√©connecte
**Solution** : Utiliser le script keep-alive
```javascript
// Dans la console du navigateur (F12)
function ClickConnect(){
    console.log("Keeping alive...");
    document.querySelector("colab-toolbar-button#connect").click()
}
setInterval(ClickConnect, 60000)
```

---

## üìö Ressources Utiles

- **Documentation Scikit-learn** : https://scikit-learn.org/
- **Documentation TensorFlow** : https://www.tensorflow.org/
- **Documentation Streamlit** : https://docs.streamlit.io/
- **CICIDS2017 Paper** : https://www.unb.ca/cic/datasets/ids-2017.html
- **MITRE ATT&CK** : https://attack.mitre.org/

---

## üéØ Prochaines √âtapes

1. ‚úÖ Setup termin√©
2. ‚è≥ Explorer le dataset sur Kaggle
3. ‚è≥ Nettoyer et pr√©parer les donn√©es
4. ‚è≥ Entra√Æner les mod√®les
5. ‚è≥ √âvaluer et comparer
6. ‚è≥ D√©ployer le dashboard

---

**Bon courage ! üöÄ**
